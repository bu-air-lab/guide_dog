<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Leveraging Human Knowledge to Learn Diverse Quadruped Locomotion Policies</title>

    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link href="/css/css" rel="stylesheet" type="text/css">

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.998.0" data-gr-ext-installed="">
    <link media="all" href="/css/glab.css" type="text/css" rel="StyleSheet">

    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }

        #primarycontent {
            MARGIN-LEFT: auto;
            WIDTH: expression(document.body.clientWidth > 500? "500px": "auto");
            MARGIN-RIGHT: auto;
            TEXT-ALIGN: left;
            max-width: 750px;
        }

        BODY {
            TEXT-ALIGN: center
        }

    </style>

    <div id="primarycontent">
        <center>
            <h0>Leveraging Human Knowledge to Learn Diverse Quadruped Locomotion Policies</h0>
        </center>
        <center>
            <p>David DeFazio,&nbsp;&nbsp;Yohei Hayamizu,&nbsp;&nbsp;Shiqi Zhang</p>
            <center>
                <p>SUNY Binghamton</p>
                <center>
                    <p>IEEE RA-L, 2023 (under review) </p>
                    <p>[<a href="#">Paper</a>]&nbsp[<a href="https://github.com/bu-air-lab/RM_Isaac">Code</a>]&nbsp[<a href="https://youtu.be/lCvopVym4q0">Video</a>]
                    </p>

                  <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td valign="middle" align="center">
                                    <iframe width="500" height="320" src="https://youtube.com/embed/lCvopVym4q0"></iframe>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <h1 align="center">Abstract</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> Legged robots are versatile in navigating challenging terrains and environments that can be difficult to other
robot platforms (e.g., the wheeled). Although there has been
success in applying reinforcement learning (RL) methods to
learn quadruped locomotion policies, there is little research
on leveraging rule-based human knowledge to facilitate this
learning process. In this paper, we develop a novel approach,
referred to as RMLL, demonstrating that human knowledge in
the form of Linear Temporal Logic formulas can be applied
to specify different locomotion gaits, and efficiently learn
these gaits via RL. This is done without the use of reference
trajectories, another form of prior knowledge commonly used
to specify gait type. To achieve this, we express our task as an
automaton constructed under a recently developed framework
called Reward Machine (RM). Experimental results in simu-
lation show that exposing the policy to the RM state allows
for better leveraging this rule-based knowledge, in terms of
improved learning efficiency. We demonstrate these learned
policies on a quadruped robot in outdoor environments.</p>
                    </div>

                    <br>

                    <h1 align="center">Overview</h1>
                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/overview.png" style="width:100%;margin-left:0%;margin-right:0%;">
                        </td>
                    </table>

                    <div style="font-size:30px">
                        <p align="justify" width="20%">
                            Overview of RM-based Locomotion Learning (RMLL). We consider propositional statements specifying foot contacts. We then construct an automaton via LTL formulas over propositional statements for each locomotion gait (left side). We follow the RM framework to convert an environment with non-Markovian rewards to a cross-product MDP, constructed via the environment and automaton. Our agent then learns from this gait-specific, cross-product MDP (right side)
                        </p>
                    </div>

                    <br>

                    <h1 align="center">Experiment Results</h1>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td><img src="./img/plot_trot.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_bound.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_pace.png" width="300" height="250">
                                </td>
                            </tr>
                        </tbody>
                    </table>


                   <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td><img src="./img/plot_walk.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_Three-One.png" width="300" height="250">
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <div style="font-size:30px">
                        <p >
		Reward curves for all gaits. RMLL more efficiently accumulates reward for each gait, particularly for the more complex gaits Walk and Three-One.
                        </p>
                    </div>
			

                    <h1 align="center">Demonstrations</h1>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td>
					<center><h3>Trot</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/trot_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Bound</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/bound_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Pace</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/pace_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                   <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td>
					<center><h3>Walk</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/walk_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Three-One</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/three_one_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <center>
                        <h1>Acknowledgements</h1>
                    </center>
                    <!-- The webpage template was borrowed from some <a href="https://nvlabs.github.io/SPADE/">GAN folks</a>. -->
                    <div style="font-size:30px">
                        <p align="justify">
                            This work has taken place in the Autonomous Intelligent Robotics (AIR) Group at SUNY Binghamton. AIR research is supported in part by grants from the National Science Foundation (IIS-1925044 and REU Supplement), Ford Motor Company (URP Awards), OPPO (Faculty Research Award), and SUNY Research Foundation
                        </p>
                    </div>
                    <br><br>

</html>
