<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control</title>

    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link href="/css/css" rel="stylesheet" type="text/css">

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.998.0" data-gr-ext-installed="">
    <link media="all" href="/css/glab.css" type="text/css" rel="StyleSheet">

    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }

        #primarycontent {
            MARGIN-LEFT: auto;
            WIDTH: expression(document.body.clientWidth > 500? "500px": "auto");
            MARGIN-RIGHT: auto;
            TEXT-ALIGN: left;
            max-width: 750px;
        }

        BODY {
            TEXT-ALIGN: center
        }

    </style>

    <div id="primarycontent">
        <center>
            <h0>Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control</h0>
        </center>
        <center>
            <p>David DeFazio,&nbsp;&nbsp;Eisuke Hirota,&nbsp;&nbsp;Shiqi Zhang</p>
            <center>
                <p>SUNY Binghamton</p>
                <center>
                    <p>CoRL, 2023</p>
                    <p>[<a href="#">Paper</a>]&nbsp[<a href="https://github.com/bu-air-lab/guide_dog">Code</a>]&nbsp[<a href="https://youtu.be/NdmpslnWkUE">Video</a>]
                    </p>

                  <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td valign="middle" align="center">
                                    <iframe width="500" height="320" src="https://youtube.com/embed/NdmpslnWkUE"></iframe>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <h1 align="center">Abstract</h1>
                    <div style="font-size:30px">
                        <p align="justify" width="20%"> Seeing-eye robots are very useful tools for guiding visually impaired people, po-
                            tentially producing a huge societal impact given the low availability and high cost
                            of real guide dogs. Although a few seeing-eye robot systems have already been
                            demonstrated, none considered external tugs from humans, which frequently oc-
                            cur in a real guide dog setting. In this paper, we simultaneously train a locomo-
                            tion controller that is robust to external tugging forces via Reinforcement Learn-
                            ing (RL), and an external force estimator via supervised learning. The controller
                            ensures stable walking, and the force estimator enables the robot to respond to
                            the external forces from the human. These forces are used to guide the robot to
                            the global goal, which is unknown to the robot, while the robot guides the human
                            around nearby obstacles via a local planner. Experimental results in simulation
                            and on hardware show that our controller is robust to external forces, and our
                            seeing-eye system can accurately detect force direction. We demonstrate our full
                            seeing-eye robot system on a real quadruped robot with a blindfolded human.</p>
                    </div>

                    <br>

                    <h1 align="center">Overview</h1>
                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <td>
                            <img src="./img/overview_fig-1-1.png" style="width:100%;margin-left:0%;margin-right:0%;">
                        </td>
                    </table>

                    <div style="font-size:30px">
                        <p align="justify" width="20%">
                            Overview of our approach. Our locomotion controller (circled in red) contains a velocity
estimator, force estimator, and locomotion policy, all of which are trained in simulation. The base
velocity estimator and force estimator are trained via supervised learning, using privileged infor-
mation from the simulator as labels. The locomotion policy is trained via RL, and outputs target
joint angles to a PD controller which converts them to joint torques which are directly applied to
the robot. During deployment, our locomotion controller estimates external force at each time step.
Force direction is derived from peaks in the estimated force signal. The direction of force determines
the next local navigation goal for our navigation system to take, which returns velocity commands
to our controller.
                        </p>
                    </div>

                    <br>

                    <!-- <h1 align="center">Experiment Results</h1>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td><img src="./img/plot_trot.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_bound.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_pace.png" width="300" height="250">
                                </td>
                            </tr>
                        </tbody>
                    </table>


                   <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td><img src="./img/plot_walk.png" width="300" height="250">
                                </td>
                                <td><img src="./img/plot_Three-One.png" width="300" height="250">
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <div style="font-size:30px">
                        <p >
		Reward curves for all gaits. RMLL more efficiently accumulates reward for each gait, particularly for the more complex gaits Walk and Three-One.
                        </p>
                    </div> -->
			

                    <!-- <h1 align="center">Demonstrations</h1>

                    <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td>
					<center><h3>Trot</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/trot_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Bound</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/bound_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Pace</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/pace_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                   <table border="0" cellspacing="10" cellpadding="0" align="center">
                        <tbody>
                            <tr>
                                <td>
					<center><h3>Walk</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/walk_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                                <td>
					<center><h3>Three-One</h3></center>
					<video width="320" height="240" controls>
					<source src="./img/three_one_vid_small.mp4" type="video/mp4">
					</video>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <center>
                        <h1>Acknowledgements</h1>
                    </center>
                    <!-- The webpage template was borrowed from some <a href="https://nvlabs.github.io/SPADE/">GAN folks</a>. -->
                    <div style="font-size:30px">
                        <p align="justify">
                            This work has taken place in the Autonomous Intelligent Robotics (AIR) Group at SUNY Binghamton. AIR research is supported in part by grants from the National Science Foundation (IIS-1925044 and REU Supplement), Ford Motor Company (URP Awards), OPPO (Faculty Research Award), and SUNY Research Foundation
                        </p>
                    </div> -->
                    <br><br>

</html>
